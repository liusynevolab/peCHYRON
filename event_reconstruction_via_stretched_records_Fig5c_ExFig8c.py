# This script was used to analyze the data in Figure 5c and Extended Data Figure 8c.

# As an input, the script reads in signatures_dataframes_removed_unexpected_sigs.csv files, which are generated by running remove_unexpected_sigs_Fig5c_ExFig8c.py.

# All the records with 2, 3, 4, or 5 insertions are stretched to an equal length, then pooled together, and the fraction of each signature at each position is reported in the [sample name]_STRETCHED_RECORDS_all2-5ins.csv output file.
# Similarly, all records with 2, 3, or 4 insertions and stretched, pooled, and the fraction of each signature is reported in [sample name]_STRETCHED_RECORDS_all2-4ins-csv. This output was necessary for samples that had fewer than 100 reads with 5 insertions.
# For quality control, the composition of signatures in stretched records for each individual insertion length are also outputted (e.g., all records with exactly 1 insertion are stretched and pooled together in one output, all the records with exactly 2 insertions are stretched and pooled together in a separate output, etc.).

import sys
import os
import re
import numpy as np
import collections
from collections import defaultdict
import os.path
import pandas as pd
import math
from math import gcd
import csv

### USER INPUTS: ###

signatures = {'C-odd': 'C',   #this dictionary converts 1-character signatures to the signals they record (e.g., a "C" signature in an odd position and a "c" signature in an even position both correspond to the constitutively expressed pegRNAs (signal = "C")).
              'c-even': 'C',  #specifying whether the signatures are in odd (A->B) or even (B->A) positions allows you to use the same signature for different signals (e.g., if you wanted, a signature "x" on an A->B pegRNA could be induced by dox, while the same signature "x" on a B->A pegRNA could be induced by IPTG).
              'L-odd': 'I',     #each signal should have 2 signatures (one odd for A->B and one even for B->A) corresponding to it.
              'l-even': 'I',
              'T-odd': 'D', 
              't-even': 'D'}         
short_list_signals = ['C', 'I', 'D'] #give a list of the signals that are being recorded (1 letter represents each signal)
lists_of_lengths_to_tally = [[1],[2],[3],[4],[5], [2,3,4,5], [2,3,4]] #what length records (how many insertions) do you want to be stretched + summed together?  Multiple lengths within one list will be summed together
values_for_lcm_calc = [2,3,4,5] #what set of numbers should be used to calculate the length to which all records should be stretched (final length = least common multiple of all lengths in the pool)

base_input_file_name = 'PER71-'     #what's the constant part of the fastq file names? In our fastq files, this string immediately precedes the sample number. Here you are providing the names of the input files.
input_identifiers = [3,4]   #what sample numbers are you analyzing?  You can manually type in a list of sample numbers and comment out the For loop below, or leave this list empty and populate it with the For loop below.
#for num in range(27,81):
#    input_identifiers.append(num)


########## END OF USER INPUTS ##########
########## READING IN EXPERIMENTAL DATA ##########
for file in input_identifiers:
    length_counter = 1 #this counter will specify the appropriate name for the output file.  It describes which lengths were being stretched + summed together.
    for lengths_to_tally in lists_of_lengths_to_tally:
        F_sample_file = open("{}{}_signatures_dataframe_removed_unexpected_sigs.csv".format(base_input_file_name, file),'r') 
        raw_data = pd.read_csv(F_sample_file)
        data = raw_data.dropna(axis=0) #removing rows that have NaN (it's just one row, because wt records are denoted as an empty string).  This avoids errors, plus we don't want to stretch the wt records.

        edit_labels = 'record' #this is the header in the input file for the column containing records
        list_of_edit_types = []
        for row in data.loc[:, edit_labels]: #making a list of the records
            list_of_edit_types.append(row)

        epoch_combo = "counts" #this is the header in the input file for the column containing counts of each record
        list_of_edit_proportions = []
        correct_header = '{epochs}'.format(epochs = epoch_combo) 
        for row in data.loc[:, correct_header]:
            list_of_edit_proportions.append(row) #making a list of the counts of each record


        ######### CONVERTING RAW RECORDS TO 'sig-odd sig-even' format, then converting the signatures to their corresponding signals ##########

        list_of_odd_v_even_edit_types = [] 
        for record in list_of_edit_types:
            counter = 1
            converted_record = []
            for ins in record:
                if counter%2 == 1:
                    converted_ins = ins + '-odd'
                else:
                    converted_ins = ins + '-even'
                converted_record.append(converted_ins)
                counter += 1
            list_of_odd_v_even_edit_types.append(converted_record)

        list_of_edits_wrt_signals = []
        for record in list_of_odd_v_even_edit_types: # converting 'sig-odd' and 'sig-even' to the one-letter symbol corresponding to the appropriate signal
            converted_record = ''
            for ins in record:
                if ins in signatures.keys():
                    old_ins = ins
                    ins = signatures[old_ins]
                else:
                    ins = '.' #if the insertion doesn't correspond to an expected signature, replace it with a character designated for unknown signatures
                converted_record += ins
            list_of_edits_wrt_signals.append(converted_record)
                    

        ### "STRETCHING OUT" RECORDS AND ADDING THEM TOGETHER TO DO SIGNAL RECONSTRUCTION ###

        compare = 1 #this is a starting point to find the longest record length that should be used to calculate the least common multiple of all record lengths
        for val in values_for_lcm_calc:
                if val > compare:
                    compare = val

        longest_length = compare

        list_of_lengths = []
        for num in range(1, longest_length +1):
            list_of_lengths.append(num)

        lcm = 1
        for i in list_of_lengths:
            lcm = lcm*i // gcd(lcm,i) #calculating the least common multiple of the record lengths.  All records will be stretched to this length.

        x_axis = lcm

        df_length_frequencies = []
        list_length_frequencies = [] #for each individual read length, I'm adding up the frequency of all records of that length.  This will be used for weighting.
        for num in lengths_to_tally:
            list_length_frequencies.append(num)
            frequency_of_length_L = 0
            for record,frequency in zip(list_of_edit_types, list_of_edit_proportions):
                if len(record) == num:
                    frequency_of_length_L += frequency
            list_length_frequencies.append(frequency_of_length_L)
            df_length_frequencies.append(list_length_frequencies)
            list_length_frequencies = []

        max_frequency = 0
        for length in df_length_frequencies: #figuring out the read length with the highest frequency, to determine how to weight the other read lengths (after weighting, we want an equal number of reads of each length)
            if length[1] > max_frequency:
                max_frequency = length[1]

        if max_frequency == 0: #if there are no records corresponding to the length of interest, skip this length and move on to the next
            length_counter += 1
            continue

        list_stretched_recs = []
        for record, frequency in zip(list_of_edits_wrt_signals, list_of_edit_proportions): #stretching out each record to a consistent length
            if len(record) in lengths_to_tally: #filter out records that have more or less insertions than we want to consider
                raw_length = len(str(record)) 
                stretch_factor = int(x_axis / raw_length) 
                
                stretched_rec = ''
                for ins in record:
                    stretched_rec = stretched_rec + (ins*stretch_factor) #stretching the record
                
                weight = 0
                for length in df_length_frequencies: #figuring out what weight to assign to the record, considering its length
                    if len(record) == length[0]:
                        if length[1] != 0:
                            weight = max_frequency / length[1]
                        else:
                            weight = 0
                        list_stretched_recs.append([frequency*weight, stretched_rec])
                
            
        record_data = []
        record_collection = []    
        for signal in short_list_signals: #I'm tallying up ALL stretched records.  One list is being made for the counts of each signature in each stretched record.  This will be compiled into a pandas dataframe and summed up to get total counts of each signature.
            for record in list_stretched_recs:
                record_data.append(signal)
                for ins in record[1]:
                    if ins == signal:
                        record_data.append(record[0])
                    else:
                        record_data.append(0)
                record_collection.append(record_data)
                record_data = []

        output_lengths = 'undefined'
        if length_counter == 1:
            output_lengths = '1ins'
        if length_counter == 2:
            output_lengths = '2ins'
        if length_counter == 3:
            output_lengths = '3ins'
        if length_counter == 4:
            output_lengths = '4ins'
        if length_counter == 5:
            output_lengths = '5ins'
        if length_counter == 6:
            output_lengths = 'all2-5ins'
        if length_counter == 7:
            output_lengths = 'all2-4ins'   
            
        stretched_dataframe = pd.DataFrame(record_collection)
        summed_stretched_dataframe = stretched_dataframe.groupby(by=[0]).sum().to_csv('{}{}_STRETCHED_RECORDS_{}.csv'.format(base_input_file_name, file, output_lengths)) #before exporting to Excel, sum up all of the counts of each pair at each position along the x-axis

        F_sample_file.close()
        length_counter += 1